{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "venv-torch",
   "display_name": "fastai v1"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1\n",
    "## Question:\n",
    "Consider the diagrams in Figure 5.1. \n",
    "\n",
    "<img src=\"img/501.png\" style=\"height:350px\">\n",
    "\n",
    "Why does the estimated value function jump up for the last two rows in the rear? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?\n",
    "## Answer:\n",
    "The reason for the estimated value function to jump up for the last two rows in the rear is that the player sticks on 20 or 21, and hence has a much higher probability to win the game. The estimated value function drops off for the whole last row on the left because the dealer is in posession of an ace card, which decreases the probability for the player to win. The frontmost values are higher in the upper diagrams than in the lower ones because in the upper diagrams, the player has a \"usable\" ace card, which again increases the probability for the player to win."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2\n",
    "## Question:\n",
    "Suppose every-visit MC was used instead of first-visit MC on the blackjack task. Would you expect the results to be very different? Why or why not?\n",
    "\n",
    "## Answer:\n",
    "The results should be similar: all the rewards are zero (except for the last step). So in an episode, the return will only be changed in the last step, no matter if the method is first-visit or every-visit."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.3\n",
    "## Question:\n",
    "What is the backup digram for Monte Carlo estimation of $q_{\\pi}$?\n",
    "\n",
    "## Answer:\n",
    "<img src=\"img/503.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.4\n",
    "## Question:\n",
    "The pseudocode for Monte Carlo ES is inefficient because, for each state-action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state-action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this.\n",
    "\n",
    "## Answer:\n",
    "\\begin{aligned} &\\text{Initialize:} \\\\ \n",
    "&\\qquad \\pi(s) \\in \\mathcal A(s) \\text{(arbitrarily), for all } s \\in S \\\\ \n",
    "&\\qquad Q(s, a) \\in \\mathbb R \\text{(arbitrarily), for all } s \\in S, a \\in \\mathcal A(s) \\\\ \n",
    "&\\qquad counts(s, a) \\leftarrow 0\\text{, for all s } \\in S, a \\in \\mathcal A(s) \\\\ \n",
    "&\\text{Loop forever (for each episode):} \\\\ \n",
    "&\\qquad \\text{Choose }S_0 \\in \\mathcal S, A_0 \\in \\mathcal A(S_0) \\text{ randomly such that all pairs have probability} > 0 \\\\ \n",
    "&\\qquad \\text{Generate an episode from }S_0, A_0, \\text{following }\\pi: S_0, A_0, R_1, . . . , S_{T -1}, A_{T-1}, R_T \\\\ \n",
    "&\\qquad G \\leftarrow 0 \\\\ \n",
    "&\\qquad \\text{Loop for each step of episode, } t = T -1, T -2, . . . , 0: \\\\ \n",
    "&\\qquad \\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\ \n",
    "&\\qquad \\qquad \\text{Unless the pair }S_t, A_t \\text{ appears in }S_0, A_0, S_1, A_1 . . . , S_{t-1}, A_{t-1}: \\\\ \n",
    "&\\qquad \\qquad \\qquad counts(S_t,A_t) \\leftarrow counts(S_t,A_t) + 1\\\\ \n",
    "&\\qquad \\qquad \\qquad Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac {(G - Q(S_t, A_t))}{count(S_t, A_t)} \\\\ \n",
    "&\\qquad \\qquad \\qquad \\pi(S_t) \\leftarrow \\text{argmax}_a Q(S_t, a) \\\\ \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.5\n",
    "## Question: \n",
    "Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability $p$ and transitions to the terminal state with probability $1 - p$. Let the reward be $+1$ on all transitions, and let $\\gamma = 1$. Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?\n",
    "\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.6\n",
    "## Question:\n",
    "What is the equation analogous to (5.6) for <i>action</i> values $Q(s,a)$ instead of state values $V(s)$, again given returns generated using $b$?\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.7\n",
    "## Question:\n",
    "In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.8\n",
    "## Question:\n",
    "The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not? \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.9\n",
    "## Question:\n",
    "Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.4.\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.10\n",
    "## Question:\n",
    "Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3).\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.11\n",
    "## Question:\n",
    "In the boxed algorithm for off-policy MC control, you may have been expecting the $W$ update to have involved the importance-sampling ratio $\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$, but instead it involves $\\frac{1}{b(A_t|S_t)}$. Why is this nevertheless correct?\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.12 <i>Racetrack</i> (programming)\n",
    "## Question:\n",
    "Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at ine of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by $+1$, $-1$, or $0$ in each time step, for a total of nine ($3$ x $3$) actions. Both velocity components are restricted to be nonnegative and less than $5$, and they cannot both be zero except at the starting line. Each eposiode begins in one of the randomly selected start states with both velocity components zero, and ends when the car crosses the finish line. The rewards are $-1$ for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car's location at each step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories). \n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Exercise 5.13\n",
    "## Question:\n",
    "Show the steps to derive (5.14) from (5.12).\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Exercise 5.14\n",
    "## Question:\n",
    "Modify the algorithm for off-policy Monte Carlo control (page 111) to use the idea of the truncated weighted-average estimator (5.10). Note that you will first need to convert this equation to action values.\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blackjack (programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjack\n",
    "\n",
    "\n",
    "# Set the absolute path to the irectory where files are/will be stored.\n",
    "abs_path = \"C:/Temp/rlai-exercises/Chapter 5/data\"\n",
    "\n",
    "# Set the number of episodes (= Blackjack games) to be simulated.\n",
    "num_episodes = 500000\n",
    "\n",
    "# Set the epsilon for the epsilon-greedy policy in Monte Carlo On-Policy Control.\n",
    "epsilon = 0.0001\n",
    "\n",
    "# Choose which algorithm(s) to execute and whether to use cached files from disk. \n",
    "blackjack.compute_prediction(num_episodes, episodes_from_disk=False, stats_from_disk=False)\n",
    "blackjack.compute_control_ES(num_episodes, stats_from_disk=False)\n",
    "blackjack.compute_control_on_policy(num_episodes, epsilon, stats_from_disk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}