{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11,down)$?\n",
    "What is $q_{\\pi}(7,down)$?\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(11,down) = 1 \\cdot (-1 + 1 \\cdot 0) = 0 \\\\\n",
    "q_{\\pi}(7,down) = 1 \\cdot (-1 + 1 \\cdot (-14)) = -14\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, suppose a new state $15$ was added to the gridworld just below state $13$, and its actions, $left$, $up$, $right$ and $down$, take the agent to states $12$, $13$, $14$ and $15$, respectively. Assume that the transitions ___from___ the original states are unchanged. What, then, is $v_{\\pi}(15)$ for the equiprobable random policy? Now suppose the dynamics of state $13$ are also changed, such that action $down$ from state $13$ takes the agent to the new state $15$. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "## Answer:\n",
    "With still unchanged transitions ___from___ the original states, there is no way to get to state $15$ unless already in it (because $p(13|13,down) = 1$ and $p(15|13,down) = 0$). Therefore, no other state's value is dependent upon state $15$ than state $15$ itself.\n",
    "\n",
    "The general equation for state value is, in this example:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\\\ = \\frac{1}{4} \\sum_{a}[v_{\\pi}(s') - 1]\n",
    "\\end{equation*}\n",
    "\n",
    "Seven state value update iterations using the below equation are sufficient for $v(15)$ to clearly approximate -20:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(15) = \\frac{1}{4} \\big( v_{\\pi}(12) - 1 + [v_{\\pi}(13) - 1 + [v_{\\pi}(14) - 1 + [v_{\\pi}(15) - 1 \\big) \\\\\n",
    "v_{1}(15) = -15 \\\\\n",
    "v_{2}(15) = -18.75 \\\\\n",
    "v_{3}(15) = -19.6875 \\\\\n",
    "v_{4}(15) = -19.921875 \\\\\n",
    "v_{5}(15) = -19.98046875 \\\\\n",
    "v_{6}(15) = -19.9961171875 \\\\\n",
    "v_{7}(15) = -19.998779296875 \\approx -20 \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "With the change in dynamics of state $13$, there is now a path to state $15$ - from state $13$. The value of state $13$ (and, subsequently, of all of its possible predecessor states) would need to be updated to reflect the new dynamics. To update the value state from $v_{\\pi, old}(13)$ to $v_{\\pi, new}(13)$, one would only need to substitute $v_{\\pi}(13)$ by $v_{\\pi}(15)$ to reflect the action $down$ - the rest of the calculation remains the same. However, the problem states $v_{*}(13)= -20$ and we already arrived at $v_{*}(15) = -20$. Hence, the value of state $13$ will remain effectively unchanged, and so will its possible predecessor states' values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3\n",
    "\n",
    "## Question:\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the actoin-value function $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$, ...? \n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\mathbb{E}[G_t | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a') \\big] \\\\\n",
    "q_{k+1}(s,a) = \\mathbb{E}[R_{t+1} + \\gamma q_{k}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{k}(s',a') \\big] \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.4\n",
    "\n",
    "## Question:\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so convergence is guaranteed.\n",
    "\n",
    "## Answer:\n",
    "The corrected algorithm is:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.5\n",
    "\n",
    "## Question:\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of this book.\n",
    "\n",
    "## Answer:\n",
    "The algorithm for finding optimal action-values is:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$Q(s,a) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$, $a \\in A(s)$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S$, $a \\in A(s)$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$q \\leftarrow Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$Q(s,a) \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma Q(s',\\pi(s))]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |q - Q(s,a)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma Q(s',\\pi(s))]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $Q \\approx q_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.6\n",
    "\n",
    "## Question:\n",
    "\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$___-soft___, meaning that the probability of selecting each item in each state, $s$, is at least $\\frac{\\epsilon}{|A(s)|}$. Describe qualitatively the changes that would be required in each of the steps $3$, $2$, and $1$, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "## Answer:\n",
    "The necessary changes to the algorithm are:\n",
    "1. Initialization\n",
    "\n",
    "   <div style=\"background-color:#ffffaa\">$V(s) \\in \\mathbb{R}$ arbitrarily for all $s \\in S$</div>\n",
    "   <div style=\"background-color:#ffffaa\">$\\pi(s) \\leftarrow $ random action from $A(s)$ with $p = \\frac{1}{|A(s)|}$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s)$</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow a_* = argmax_{a} \\sum_{s',r} p(s',r|s,a) [r + \\gamma V(s')]$, with $p = 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$, or</div>\n",
    "   <div style=\"background-color:#ffffaa\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow$ any $a \\neq a_*$, with $p = \\frac{\\epsilon}{|A(s)|}$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.7 (programming)\n",
    "\n",
    "## Question:\n",
    "Write a program for policy iteration and re-solve Jack's car rental problem with the following changes. One of Jack's employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), than an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of non-linearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem. \n",
    "\n",
    "## Answer:\n",
    "### Discussion and Assumptions:\n",
    "1. The problem statement does not explicitly mention the use of an <b>$\\epsilon$-soft policy</b>. The below code assumes it and sets the value of $\\epsilon$ in <b>CarRental/constants.py</b>. Furthermore, our understanding of an $\\epsilon$-soft policy is:\n",
    " - during <b>Initialization</b> and <b>Policy Improvement</b>, we compute the policy function $\\pi(a|s)$, assigning probability $\\pi(a_*|s) = 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$ for the value-maximizing action $a_*$ of state $s$ and probability $\\pi(a_{other}|s) = \\frac{\\epsilon}{|A(s)|}$ for every other action $a_{other}$ available for state $s$;\n",
    " - during <b>Policy Evaluation</b>, we compute the value function $v(s)$ by using an <b>$\\epsilon$-greedy policy</b>, which ensures $\\epsilon$ probability that any valid action will be selected for each state. This measure of exploration should provide theoretical guarantees for convergence.\n",
    "\n",
    "\n",
    "2. The Problem statement doesn't mention the value, $\\theta$, of the threshold for further refreshes of the value function. The below code sets the value of $\\theta$ in <b>CarRental/constants.py</b>.\n",
    "\n",
    "\n",
    "3. Because the number of rentals/returns at any location is limited to a maximum of 20 cars, the Poisson probabilities of having any number of rentals or returns at any location in a business day don't add up to 1. The below code addresses this by padding the probability of the highest possible number of rentals/returns in any given situation. This way, we theoretically:\n",
    " - handle 12 returns at a location with leftover capacity of 3 returns as part of the probability of having only 3 returns that day;\n",
    " - handle 12 rentals at a location with only 3 available cars as part of the probability of having rented only 3 cars that day (with no rewards for the extra rental requests that couldn't be fulfilled).\n",
    " \n",
    " \n",
    "4. We further assume that, per the extended problem statement (Exercise 4.7), a maximum of 5 cars can be moved overnight from location A to location B (1 shuttled by an employee at no cost and up to 4 for $2 a piece). In the original problem (as stated in Example 4.2), every moved car would incur cost. \n",
    "\n",
    "\n",
    "5. The code is based on the concept of a <b>pseudo-state</b>, which can be intuitively described as the number of cars across locations at 6am: following transfer of cars but prior to the start of the business day. When in this pseudo-state, which state is reached next depends on Poisson random variables.\n",
    "\n",
    "### Instructions for running the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducing the original Jack's Car Rental Problem (Example 4.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\ninitialized dataframe dfPi 18:49:05\n(re)set index for dfPi 18:49:05\ninitialized dataframe dfV 18:49:05\n(re)set index for dfV 18:49:05\nvalues deemed not good enough, going for another value loop 19:03:06\n         s      v_of_s\n0    00_00   -0.001203\n1    00_01    9.804057\n2    00_02   18.988893\n3    00_03   26.803847\n4    00_04   32.666487\n5    00_05   36.553367\n6    00_06   38.894053\n7    00_07   40.154966\n8    00_08   40.741600\n9    00_09   40.899297\n10   00_10   40.741645\n11   00_11   40.327106\n12   00_12   39.733764\n13   00_13   39.080186\n14   00_14   38.487712\n15   00_15   38.033808\n16   00_16   37.742179\n17   00_17   37.609230\n18   00_18   37.622670\n19   00_19   37.729964\n20   00_20   37.825923\n21   01_00   10.239673\n22   01_01   20.226292\n23   01_02   29.976477\n24   01_03   38.533118\n25   01_04   45.046744\n26   01_05   49.479980\n27   01_06   52.325339\n28   01_07   54.045048\n29   01_08   55.015864\n..     ...         ...\n411  19_12  176.102251\n412  19_13  177.003123\n413  19_14  177.510489\n414  19_15  177.846935\n415  19_16  178.275086\n416  19_17  179.159512\n417  19_18  180.869184\n418  19_19  183.098156\n419  19_20  184.679376\n420  20_00  102.801286\n421  20_01  113.390277\n422  20_02  124.546073\n423  20_03  135.432050\n424  20_04  145.445680\n425  20_05  154.477135\n426  20_06  162.519267\n427  20_07  169.511212\n428  20_08  175.394646\n429  20_09  180.151435\n430  20_10  183.802576\n431  20_11  186.418861\n432  20_12  188.145735\n433  20_13  189.199172\n434  20_14  189.818989\n435  20_15  190.220827\n436  20_16  190.614591\n437  20_17  191.162904\n438  20_18  191.995881\n439  20_19  192.956534\n440  20_20  193.591697\n\n[441 rows x 2 columns] 193.59169735631147\nvalues deemed not good enough, going for another value loop 19:19:36\n         s      v_of_s\n0    00_00   45.026305\n1    00_01   55.250926\n2    00_02   65.552804\n3    00_03   75.402421\n4    00_04   84.379397\n5    00_05   92.280749\n6    00_06   99.027956\n7    00_07  104.553219\n8    00_08  108.871488\n9    00_09  112.044167\n10   00_10  114.160301\n11   00_11  115.343908\n12   00_12  115.777894\n13   00_13  115.699014\n14   00_14  115.350486\n15   00_15  114.932150\n16   00_16  114.588406\n17   00_17  114.428187\n18   00_18  114.514677\n19   00_19  114.773832\n20   00_20  114.987925\n21   01_00   56.938859\n22   01_01   67.706980\n23   01_02   79.032720\n24   01_03   89.796567\n25   01_04   99.311849\n26   01_05  107.523589\n27   01_06  114.547083\n28   01_07  120.384911\n29   01_08  125.055616\n..     ...         ...\n411  19_12  266.577150\n412  19_13  269.738895\n413  19_14  272.285268\n414  19_15  274.399376\n415  19_16  276.274965\n416  19_17  278.093675\n417  19_18  280.000830\n418  19_19  281.760852\n419  19_20  282.785130\n420  20_00  172.457816\n421  20_01  182.878057\n422  20_02  193.671963\n423  20_03  204.350213\n424  20_04  214.628230\n425  20_05  224.451541\n426  20_06  233.755179\n427  20_07  242.418925\n428  20_08  250.317048\n429  20_09  257.340453\n430  20_10  263.402787\n431  20_11  268.467121\n432  20_12  272.583896\n433  20_13  275.893081\n434  20_14  278.576615\n435  20_15  280.800968\n436  20_16  282.715449\n437  20_17  284.373430\n438  20_18  285.782837\n439  20_19  286.815286\n440  20_20  287.306555\n\n[441 rows x 2 columns] 111.55873479661753\nvalues deemed not good enough, going for another value loop 19:37:05\n         s      v_of_s\n0    00_00   95.375080\n1    00_01  105.659769\n2    00_02  116.166116\n3    00_03  126.471067\n4    00_04  136.285474\n5    00_05  145.499118\n6    00_06  154.035802\n7    00_07  161.758450\n8    00_08  168.554596\n9    00_09  174.333163\n10   00_10  179.031816\n11   00_11  182.645394\n12   00_12  185.262665\n13   00_13  187.065239\n14   00_14  188.276892\n15   00_15  189.105381\n16   00_16  189.718567\n17   00_17  190.251331\n18   00_18  190.787795\n19   00_19  191.279327\n20   00_20  191.565205\n21   01_00  107.550882\n22   01_01  118.415006\n23   01_02  129.987809\n24   01_03  141.195140\n25   01_04  151.475112\n26   01_05  160.876236\n27   01_06  169.523159\n28   01_07  177.361346\n29   01_08  184.303257\n..     ...         ...\n411  19_12  325.789092\n412  19_13  330.231417\n413  19_14  334.096452\n414  19_15  337.505176\n415  19_16  340.555236\n416  19_17  343.283404\n417  19_18  345.671125\n418  19_19  347.476384\n419  19_20  348.362640\n420  20_00  226.021033\n421  20_01  236.341893\n422  20_02  246.942083\n423  20_03  257.439448\n424  20_04  267.612981\n425  20_05  277.426336\n426  20_06  286.839515\n427  20_07  295.766871\n428  20_08  304.108757\n429  20_09  311.764871\n430  20_10  318.641363\n431  20_11  324.681440\n432  20_12  329.906440\n433  20_13  334.419470\n434  20_14  338.357340\n435  20_15  341.829465\n436  20_16  344.903305\n437  20_17  347.529414\n438  20_18  349.595808\n439  20_19  350.926406\n440  20_20  351.463073\n\n[441 rows x 2 columns] 79.51227893525515\nvalues deemed not good enough, going for another value loop 19:54:06\n         s      v_of_s\n0    00_00  142.167381\n1    00_01  152.435564\n2    00_02  162.929131\n3    00_03  173.283975\n4    00_04  183.271394\n5    00_05  192.829040\n6    00_06  201.916797\n7    00_07  210.422788\n8    00_08  218.239011\n9    00_09  225.256990\n10   00_10  231.377380\n11   00_11  236.543661\n12   00_12  240.785284\n13   00_13  244.222493\n14   00_14  247.019498\n15   00_15  249.326213\n16   00_16  251.248501\n17   00_17  252.846211\n18   00_18  254.122514\n19   00_19  254.994272\n20   00_20  255.382449\n21   01_00  154.190072\n22   01_01  164.990246\n23   01_02  176.456500\n24   01_03  187.621282\n25   01_04  198.007460\n26   01_05  207.697344\n27   01_06  216.831181\n28   01_07  225.370725\n29   01_08  233.232224\n..     ...         ...\n411  19_12  368.854920\n412  19_13  373.775242\n413  19_14  378.188229\n414  19_15  382.183713\n415  19_16  385.800460\n416  19_17  388.986162\n417  19_18  391.614418\n418  19_19  393.425503\n419  19_20  394.221933\n420  20_00  268.180335\n421  20_01  278.427046\n422  20_02  288.873351\n423  20_03  299.196798\n424  20_04  309.205603\n425  20_05  318.864351\n426  20_06  328.144190\n427  20_07  336.983173\n428  20_08  345.304528\n429  20_09  353.023511\n430  20_10  360.054801\n431  20_11  366.344762\n432  20_12  371.913036\n433  20_13  376.855284\n434  20_14  381.293023\n435  20_15  385.308864\n436  20_16  388.921882\n437  20_17  392.020111\n438  20_18  394.412828\n439  20_19  395.886636\n440  20_20  396.437960\n\n[441 rows x 2 columns] 63.81724418656634\nvalues deemed not good enough, going for another value loop 20:10:48\n         s      v_of_s\n0    00_00  183.261085\n1    00_01  193.493912\n2    00_02  203.919166\n3    00_03  214.213427\n4    00_04  224.177800\n5    00_05  233.766535\n6    00_06  242.960005\n7    00_07  251.671203\n8    00_08  259.813351\n9    00_09  267.290839\n10   00_10  274.007818\n11   00_11  279.902224\n12   00_12  284.989366\n13   00_13  289.366901\n14   00_14  293.167317\n15   00_15  296.496906\n16   00_16  299.399579\n17   00_17  301.847492\n18   00_18  303.740770\n19   00_19  304.935430\n20   00_20  305.408053\n21   01_00  195.022971\n22   01_01  205.721406\n23   01_02  216.995150\n24   01_03  227.991669\n25   01_04  238.295502\n26   01_05  247.986863\n27   01_06  257.202241\n28   01_07  265.917315\n29   01_08  274.065696\n..     ...         ...\n411  19_12  401.780401\n412  19_13  406.799617\n413  19_14  411.355001\n414  19_15  415.523933\n415  19_16  419.316474\n416  19_17  422.636257\n417  19_18  425.302586\n418  19_19  427.053779\n419  19_20  427.773750\n420  20_00  301.624664\n421  20_01  311.813847\n422  20_02  322.138827\n423  20_03  332.318902\n424  20_04  342.179743\n425  20_05  351.681458\n426  20_06  360.798911\n427  20_07  369.484182\n428  20_08  377.674857\n429  20_09  385.296269\n430  20_10  392.269452\n431  20_11  398.544899\n432  20_12  404.144628\n433  20_13  409.164289\n434  20_14  413.721538\n435  20_15  417.889080\n436  20_16  421.664020\n437  20_17  424.907065\n438  20_18  427.391934\n439  20_19  428.891818\n440  20_20  429.432121\n\n[441 rows x 2 columns] 50.02560418815534\n"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import car_rental\n",
    "import CarRental.constants\n",
    "\n",
    "# Optionally, set values of constants for the original problem:\n",
    "\n",
    "# whether we're solving the original problem (Example 4.2) \n",
    "# or for the additional requirements (Ex. 4.7);\n",
    "CarRental.constants.ORIGINAL_PROBLEM = True\n",
    "\n",
    "# whether to use disk to r/w the following CSV files:\n",
    "# (-) states, valid next actions and their respective pseudo-\n",
    "#     states, and the car transfer fees they incur (dfSASP.csv);\n",
    "# (-) pseudo-states, valid rental/return combinations for them,\n",
    "#     the next state they lead to, as well as the respective\n",
    "#     probabilities, rewards and overflow parking fees they\n",
    "#     incur (dfSp_Ren_Ret.csv);\n",
    "# (-) states and their respective values, as learned in\n",
    "#     policy iteration (dfV.csv);\n",
    "# (-) states and actions, plus their respective probabilities\n",
    "#     per policy, as learned in policy iteration (dfPi.csv).\n",
    "# Don't use this option if you don't have >=rw access to disk;\n",
    "CarRental.constants.USE_DISK_FOR_CSV_DATA = True\n",
    "\n",
    "# what directory to use for r/w of CSV files from/to disk\n",
    "CarRental.constants.PATH_SPRENRET_CSV = \"C:/Temp/rlai-exercises/Chapter 4/data\"\n",
    "\n",
    "# whether to load cached preprocessed data from CSV files\n",
    "# for purposes of quick visualization w/o a full code run\n",
    "# (dfSASP.csv, dfSp_Ren_Ret.csv).\n",
    "# Don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False;\n",
    "CarRental.constants.GET_DATA_FROM_CSV = True\n",
    "\n",
    "# whether to load cached models from CSV files\n",
    "# for purposes of quick visualization w/o a full code run\n",
    "# (dfPi.csv, dfV.csv).\n",
    "# Don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False;\n",
    "CarRental.constants.GET_MODEL_FROM_CSV = True\n",
    "\n",
    "# what file numbers to use for loading the models from\n",
    "# dfPi.csv, dfV.csv.\n",
    "# Set both to -1 if GET_MODEL_FROM_CSV = False or to\n",
    "# the file numbers >=0 if GET_MODEL_FROM_CSV = True;\n",
    "# don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False.\n",
    "CarRental.constants.PI_SEQ_NR = -1\n",
    "CarRental.constants.V_SEQ_NR = -1\n",
    "\n",
    "\n",
    "assert (not (\n",
    "    CarRental.constants.GET_DATA_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and (CarRental.constants.PI_SEQ_NR == -1 or CarRental.constants.V_SEQ_NR == -1))\n",
    "\n",
    "# for more constants, see CarRental/constants.py.\n",
    "\n",
    "# finally, run the code for visualizing the results:\n",
    "car_rental.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting the solution for the additional requirements (Ex. 4.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, set constants for the problem extended beyond Example 4.2 and Fig. 4.2\n",
    "# to the extra details described in Exercise 4.7 \n",
    "CarRental.constants.ORIGINAL_PROBLEM = False\n",
    "CarRental.constants.USE_DISK_FOR_CSV_DATA = True\n",
    "CarRental.constants.PATH_SPRENRET_CSV = \"C:/Temp/rlai-exercises/Chapter 4/data_add\"\n",
    "CarRental.constants.GET_DATA_FROM_CSV = False\n",
    "CarRental.constants.GET_MODEL_FROM_CSV = False\n",
    "CarRental.constants.PI_SEQ_NR = -1\n",
    "CarRental.constants.V_SEQ_NR = -1\n",
    "\n",
    "\n",
    "assert (not (\n",
    "    CarRental.constants.GET_DATA_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and (CarRental.constants.PI_SEQ_NR == -1 or CarRental.constants.V_SEQ_NR == -1))\n",
    "\n",
    "# finally, run the code for visualizing the results:\n",
    "car_rental.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.8\n",
    "\n",
    "## Question:\n",
    "Why does the optimal policy for the gambler's problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.9 (programming)\n",
    "\n",
    "## Question:\n",
    "Implement value iteration for the gambler's problem and solve it for for $p_h = 0.25$ and $p_h = 0.55$. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital 0 or 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as $\\theta \\rightarrow 0$?  \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.10\n",
    "\n",
    "## Question:\n",
    "What is the analog of the value iteration update (4.10) for action values, $q_{k+1}(s,a)$?\n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}