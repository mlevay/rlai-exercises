{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11,down)$?\n",
    "What is $q_{\\pi}(7,down)$?\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(11,down) = 1 \\cdot (-1 + 1 \\cdot 0) = 0 \\\\\n",
    "q_{\\pi}(7,down) = 1 \\cdot (-1 + 1 \\cdot (-14)) = -14\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, suppose a new state $15$ was added to the gridworld just below state $13$, and its actions, $left$, $up$, $right$ and $down$, take the agent to states $12$, $13$, $14$ and $15$, respectively. Assume that the transitions ___from___ the original states are unchanged. What, then, is $v_{\\pi}(15)$ for the equiprobable random policy? Now suppose the dynamics of state $13$ are also changed, such that action $down$ from state $13$ takes the agent to the new state $15$. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "## Answer:\n",
    "With still unchanged transitions ___from___ the original states, there is no way to get to state $15$ unless already in it (because $p(13|13,down) = 1$ and $p(15|13,down) = 0$). Therefore, no other state's value is dependent upon state $15$ than state $15$ itself.\n",
    "\n",
    "The general equation for state value is, in this example:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\\\ = \\frac{1}{4} \\sum_{a}[v_{\\pi}(s') - 1]\n",
    "\\end{equation*}\n",
    "\n",
    "Seven state value update iterations using the below equation are sufficient for $v(15)$ to clearly approximate -20:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(15) = \\frac{1}{4} \\big( v_{\\pi}(12) - 1 + [v_{\\pi}(13) - 1 + [v_{\\pi}(14) - 1 + [v_{\\pi}(15) - 1 \\big) \\\\\n",
    "v_{1}(15) = -15 \\\\\n",
    "v_{2}(15) = -18.75 \\\\\n",
    "v_{3}(15) = -19.6875 \\\\\n",
    "v_{4}(15) = -19.921875 \\\\\n",
    "v_{5}(15) = -19.98046875 \\\\\n",
    "v_{6}(15) = -19.9961171875 \\\\\n",
    "v_{7}(15) = -19.998779296875 \\approx -20 \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "With the change in dynamics of state $13$, there is now a path to state $15$ - from state $13$. The value of state $13$ (and, subsequently, of all of its possible predecessor states) would need to be updated to reflect the new dynamics. To update the value state from $v_{\\pi, old}(13)$ to $v_{\\pi, new}(13)$, one would only need to substitute $v_{\\pi}(13)$ by $v_{\\pi}(15)$ to reflect the action $down$ - the rest of the calculation remains the same. However, the problem states $v_{*}(13)= -20$ and we already arrived at $v_{*}(15) = -20$. Hence, the value of state $13$ will remain effectively unchanged, and so will its possible predecessor states' values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3\n",
    "\n",
    "## Question:\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the actoin-value function $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$, ...? \n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\mathbb{E}[G_t | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a') \\big] \\\\\n",
    "q_{k+1}(s,a) = \\mathbb{E}[R_{t+1} + \\gamma q_{k}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{k}(s',a') \\big] \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4\n",
    "\n",
    "## Question:\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so convergence is guaranteed.\n",
    "\n",
    "## Answer:\n",
    "The corrected algorithm is:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow \\max(\\Delta, |old$-$value - new$-$value|)$ across all actions</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $\\Delta > \\Theta$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.5\n",
    "\n",
    "## Question:\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of this book.\n",
    "\n",
    "## Answer:\n",
    "The algorithm for finding optimal action-values is:\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "   <div>$Q(s,a) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$, $a \\in A(s)$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S$, $a \\in A(s)$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$q \\leftarrow Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$Q(s,a) \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma Q(s',\\pi(s))]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |q - Q(s,a)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma Q(s',\\pi(s))]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow \\max(\\Delta, |old$-$value - new$-$value|)$ across all actions</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $\\Delta > \\Theta$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $Q \\approx q_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.6\n",
    "\n",
    "## Question:\n",
    "\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$___-soft___, meaning that the probability of selecting each item in each state, $s$, is at least $\\frac{\\epsilon}{|A(s)|}$. Describe qualitatively the changes that would be required in each of the steps $3$, $2$, and $1$, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "## Answer:\n",
    "The necessary changes to the algorithm are:\n",
    "1. Initialization\n",
    "\n",
    "   <div style=\"background-color:#777777\">$V(s) \\in \\mathbb{R}$ arbitrarily for all $s \\in S$</div>\n",
    "   <div style=\"background-color:#777777\"><i></i>For each $a in A(s)$:</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(a|s)$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$ for $a_* = 0$, or</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow \\frac{\\epsilon}{|A(s)|}$ for all $a \\neq a_*$, for every state $s$, where $|A(s)|$ signifies the number of valid actions for $s$.</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s, a)[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\arg\\max_{a} \\pi(a|s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;</i>For each $a in A(s)$:</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\pi(a|s)$</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$ for $a_* = argmax_{a} \\sum_{s',r} p(s',r|s,a) [r + \\gamma V(s')]$, or</div>\n",
    "   <div style=\"background-color:#777777\"><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\leftarrow \\frac{\\epsilon}{|A(s)|}$ for all $a \\neq a_*$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s, a)[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow \\max(\\Delta, |old$-$value - new$-$value|)$ across all actions</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action \\neq \\pi(s)$ and $\\Delta > \\Theta$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.7 (programming)\n",
    "\n",
    "## Question:\n",
    "Write a program for policy iteration and re-solve <b>Jack's car rental problem</b> with the following changes. One of Jack's employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\\\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), than an additional cost of \\\\$4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of non-linearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem. \n",
    "\n",
    "## Answer:\n",
    "### Discussion and Assumptions:\n",
    "1. The problem statement does not explicitly mention the use of an <b>$\\epsilon$-soft policy</b>. The below code assumes it and sets the value of $\\epsilon$ in <b>CarRental/constants.py</b>. Furthermore, our understanding of an $\\epsilon$-soft policy is:\n",
    " - during <b>Initialization</b> and <b>Policy Improvement</b>, we compute the policy function $\\pi(a|s)$, assigning probability $\\pi(a_*|s) = 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$ for the value-maximizing action $a_*$ of state $s$ and probability $\\pi(a_{other}|s) = \\frac{\\epsilon}{|A(s)|}$ for every other action $a_{other}$ available for state $s$;\n",
    " - during <b>Policy Evaluation</b>, we compute the value function $v(s)$ by using an <b>$\\epsilon$-greedy policy</b>, which ensures $\\epsilon$ probability that any valid action will be selected for each state. This measure of exploration should provide theoretical guarantees for convergence.\n",
    "\n",
    "\n",
    "2. The Problem statement doesn't mention the value, $\\theta$, of the threshold for further refreshes of the value function. The below code sets the value of $\\theta$ in <b>CarRental/constants.py</b>.\n",
    "\n",
    "\n",
    "3. Because the number of rentals/returns at any location is limited to a maximum of 20 cars, the Poisson probabilities of having any number of rentals or returns at any location in a business day don't add up to 1. The below code addresses this by padding the probability of the highest possible number of rentals/returns in any given situation. This way, we theoretically:\n",
    " - handle 12 returns at a location with leftover capacity of 3 returns as part of the probability of having only 3 returns that day;\n",
    " - handle 12 rentals at a location with only 3 available cars as part of the probability of having rented only 3 cars that day (with no rewards for the extra rental requests that couldn't be fulfilled).\n",
    " \n",
    " \n",
    "4. We further assume that, per the extended problem statement (Exercise 4.7), a maximum of 5 cars can be moved overnight from location A to location B (1 shuttled by an employee at no cost and up to 4 for \\\\$2 a piece). In the original problem (as stated in Example 4.2), every moved car would incur cost. \n",
    "\n",
    "\n",
    "5. The code is based on the concept of a <b>pseudo-state</b>, which can be intuitively described as the number of cars across locations at 6am: following transfer of cars but prior to the start of the business day. When in this pseudo-state, which state is reached next depends on Poisson random variables.\n",
    "\n",
    "### Instructions for running the code:\n",
    "The code is based on pandas/numpy, and was written with learning/demonstration in mind (as opposed to good practices/performance). \n",
    "\n",
    "A few constants can be set below (and have internal pre-sets). The logic itself is stored in the modules chapter_4/CarRental/* and chapter_4/car_rental.py; all constants are in chapter_4/CarRental/constants.py. At the very least, the working directory must be set correctly for the code to run. \n",
    "\n",
    "The code can be executed both in IPython (notebook chapter_4/Chapter 4 Exercises.ipynb) and directly (by running the module chapter_4/car_rental.py).\n",
    "\n",
    "Library versions used: python 3.7.3, numpy 1.15.4, matplotlib 3.1.0, pandas 0.24.2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reproducing the original Jack's Car Rental Problem (Example 4.2 + Figure 4.2):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import car_rental\n",
    "import CarRental.constants\n",
    "\n",
    "\n",
    "# Optionally, set values of constants for the original problem:\n",
    "\n",
    "# whether we're solving the original problem (Example 4.2) \n",
    "# or for the additional requirements (Ex. 4.7);\n",
    "CarRental.constants.IS_ORIGINAL_PROBLEM = True\n",
    "\n",
    "# whether to use disk to r/w the following CSV files:\n",
    "# (-) states, valid next actions and their respective pseudo-\n",
    "#     states, and the car transfer fees they incur (dfSASP.csv);\n",
    "# (-) pseudo-states, valid rental/return combinations for them,\n",
    "#     the next state they lead to, as well as the respective\n",
    "#     probabilities, rewards and overflow parking fees they\n",
    "#     incur (dfSp_Ren_Ret.csv);\n",
    "# (-) states and their respective values, as learned in\n",
    "#     policy iteration (dfV.csv);\n",
    "# (-) states and actions, plus their respective probabilities\n",
    "#     per policy, as learned in policy iteration (dfPi.csv).\n",
    "# Don't use this option if you don't have >=rw access to disk;\n",
    "CarRental.constants.USE_DISK_FOR_CSV_DATA = True\n",
    "\n",
    "# what directory to use for r/w of CSV files from/to disk\n",
    "CarRental.constants.PATH_SPRENRET_CSV = \"C:/Temp/rlai-exercises/Chapter 4/data\"\n",
    "\n",
    "# whether to load cached preprocessed data from CSV files\n",
    "# for purposes of quick visualization w/o a full code run\n",
    "# (dfSASP.csv, dfSp_Ren_Ret.csv).\n",
    "# Don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False;\n",
    "CarRental.constants.GET_DATA_FROM_CSV = True\n",
    "\n",
    "# whether to load cached models from CSV files\n",
    "# for purposes of quick visualization w/o a full code run\n",
    "# (dfPi.csv, dfV.csv).\n",
    "# Don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False;\n",
    "CarRental.constants.GET_MODEL_FROM_CSV = False\n",
    "\n",
    "# what file numbers to use for loading the models from\n",
    "# dfPi.csv, dfV.csv.\n",
    "# Set both to -1 if GET_MODEL_FROM_CSV = False or to\n",
    "# the file numbers >=0 if GET_MODEL_FROM_CSV = True;\n",
    "# don't set this to TRUE if USE_DISK_FOR_CSV_DATA = False.\n",
    "CarRental.constants.PI_SEQ_NR = -1\n",
    "CarRental.constants.V_SEQ_NR = -1\n",
    "\n",
    "\n",
    "assert (not (\n",
    "    CarRental.constants.GET_DATA_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and (\n",
    "        CarRental.constants.PI_SEQ_NR == -1 or CarRental.constants.V_SEQ_NR == -1)))\n",
    "\n",
    "# for more constants, see CarRental/constants.py.\n",
    "\n",
    "# finally, run the code for visualizing the results:\n",
    "car_rental.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adapting the solution for the additional requirements (Exercise 4.7):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import car_rental\n",
    "import CarRental.constants\n",
    "\n",
    "\n",
    "# Now, set constants for the problem extended beyond Example 4.2 and Fig. 4.2\n",
    "# to the extra details described in Exercise 4.7 \n",
    "CarRental.constants.IS_ORIGINAL_PROBLEM = False\n",
    "CarRental.constants.USE_DISK_FOR_CSV_DATA = True\n",
    "CarRental.constants.PATH_SPRENRET_CSV = \"C:/Temp/rlai-exercises/Chapter 4/data\"\n",
    "CarRental.constants.GET_DATA_FROM_CSV = True\n",
    "CarRental.constants.GET_MODEL_FROM_CSV = False\n",
    "CarRental.constants.PI_SEQ_NR = -1\n",
    "CarRental.constants.V_SEQ_NR = -1\n",
    "\n",
    "\n",
    "assert (not (\n",
    "    CarRental.constants.GET_DATA_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and CarRental.constants.USE_DISK_FOR_CSV_DATA == False))\n",
    "assert (not (\n",
    "    CarRental.constants.GET_MODEL_FROM_CSV == True and (\n",
    "        CarRental.constants.PI_SEQ_NR == -1 or CarRental.constants.V_SEQ_NR == -1)))\n",
    "\n",
    "# finally, run the code for visualizing the results:\n",
    "car_rental.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.8\n",
    "\n",
    "## Question:\n",
    "Why does the optimal policy for the gambler's problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n",
    "\n",
    "## Answer:\n",
    "In this problem, with p = 0.4, the coin is biased against the gambler. Because of this, the gambler will want to minimize the number of flips. If many small bets are made, loss is likely. Thus, with a capital of 50, all of it can be bet with a .4 probability of winning. On the other hand, with a capital of 51 and a bet of 1, even ending up with a loss there is still capital=50 and thus a .4 chance of winning. And ending up with a win, we end up with 52 and can bet 2 and maybe end up with 54 etc. \n",
    "\n",
    "In these cases there is a chance to get up to 75 without ever risking it all on one bet, yet we can always fall back (if we lose) on one big bet. And if we gets to 75, we can safely bet 25, possibly winning in one, while still being able to fall back to 50. It is this sort of logic which causes such big changes in the policy with small changes in stake, particularly at multiples of the negative powers of two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.9 (programming)\n",
    "\n",
    "## Question:\n",
    "Implement value iteration for the gambler's problem and solve it for for $p_h = 0.25$ and $p_h = 0.55$. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital 0 or 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as $\\theta \\rightarrow 0$?  \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gambler\n",
    "import Gambler.constants\n",
    "\n",
    "# The probability of flipping heads can be controlled here.\n",
    "Gambler.constants.PROB_HEADS = .4\n",
    "\n",
    "gambler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results stabilize as $\\Theta \\rightarrow 0$ for all values of $p_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.10\n",
    "\n",
    "## Question:\n",
    "What is the analog of the value iteration update (4.10) for action values, $q_{k+1}(s,a)$?\n",
    "\n",
    "## Answer:\n",
    "Using Bellman's optimality equation for action values as an update rule:\n",
    "\\begin{equation*}\n",
    "q_{k+1}(s,a) = \\mathbb{E} \\big[ R_{t+1} + \\gamma \\max_{a'} q_{k} (S_{t+1}, a') | S_t = s, A_t = a \\big] \\\\\n",
    "= \\sum_{s', r} p(s', r | s, a) \\big[ r + \\gamma \\max_{a'} q_{k} (s', a') \\big]\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}