{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11,down)$?\n",
    "What is $q_{\\pi}(7,down)$?\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{equation*}\n",
    "Therefore,\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(11,down) = 1 \\cdot (-1 + 1 \\cdot 0) = 0 \\\\\n",
    "q_{\\pi}(7,down) = 1 \\cdot (-1 + 1 \\cdot (-14)) = -14\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2\n",
    "\n",
    "## Question:\n",
    "In Example 4.1, suppose a new state $15$ was added to the gridworld just below state $13$, and its actions, $left$, $up$, $right$ and $down$, take the agent to states $12$, $13$, $14$ and $15$, respectively. Assume that the transitions ___from___ the original states are unchanged. What, then, is $v_{\\pi}(15)$ for the equiprobable random policy? Now suppose the dynamics of state $13$ are also changed, such that action $down$ from state $13$ takes the agent to the new state $15$. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "## Answer:\n",
    "With still unchanged transitions ___from___ the original states, there is no way to get to state $15$ unless already in it (because $p(13|13,down) = 1$ and $p(15|13,down) = 0$). Therefore, no other state's value is dependent upon state $15$ than state $15$ itself.\n",
    "\n",
    "The general equation for state value is, in this example:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\\\ = \\frac{1}{4} \\sum_{a}[v_{\\pi}(s') - 1]\n",
    "\\end{equation*}\n",
    "\n",
    "Seven state value update iterations using the below equation are sufficient for $v(15)$ to clearly approximate -20:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(15) = \\frac{1}{4} \\big( v_{\\pi}(12) - 1 + [v_{\\pi}(13) - 1 + [v_{\\pi}(14) - 1 + [v_{\\pi}(15) - 1 \\big) \\\\\n",
    "v_{1}(15) = -15 \\\\\n",
    "v_{2}(15) = -18.75 \\\\\n",
    "v_{3}(15) = -19.6875 \\\\\n",
    "v_{4}(15) = -19.921875 \\\\\n",
    "v_{5}(15) = -19.98046875 \\\\\n",
    "v_{6}(15) = -19.9961171875 \\\\\n",
    "v_{7}(15) = -19.998779296875 \\approx -20 \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "With the change in dynamics of state $13$, there is now a path to state $15$ - from state $13$. The value of state $13$ (and, subsequently, of all of its possible predecessor states) would need to be updated to reflect the new dynamics. To update the value state from $v_{\\pi, old}(13)$ to $v_{\\pi, new}(13)$, one would only need to substitute $v_{\\pi}(13)$ by $v_{\\pi}(15)$ to reflect the action $down$ - the rest of the calculation remains the same. However, the problem states $v_{*}(13)= -20$ and we already arrived at $v_{*}(15) = -20$. Hence, the value of state $13$ will remain effectively unchanged, and so will its possible predecessor states' values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3\n",
    "\n",
    "## Question:\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the actoin-value function $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$, ...? \n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\mathbb{E}[G_t | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t = a] \n",
    "\\\\ =  \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a') \\big] \\\\\n",
    "q_{k+1}(s,a) = \\mathbb{E}[R_{t+1} + \\gamma q_{k}(S_{t+1}, A_{t+1}) | S_t=s, A_t = a] \n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\sum_{a'} \\pi(a'|s') q_{k}(s',a') \\big] \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.4\n",
    "\n",
    "## Question:\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so convergence is guaranteed.\n",
    "\n",
    "## Answer:\n",
    "The corrected algorithm is:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action < \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.5\n",
    "\n",
    "## Question:\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of this book.\n",
    "\n",
    "## Answer:\n",
    "The algorithm for finding optimal action-values is:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$Q(s,a) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$, $a \\in A(s)$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S$, $a \\in A(s)$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$q \\leftarrow Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$Q(s,a) \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |q - Q(s,a)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow argmax_{a} Q(s,a)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action < \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $Q \\approx q_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.6\n",
    "\n",
    "## Question:\n",
    "\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$___-soft___, meaning that the probability of selecting each item in each state, $s$, is at least $\\frac{\\epsilon}{|A(s)|}$. Describe qualitatively the changes that would be required in each of the steps $3$, $2$, and $1$, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "## Answer:\n",
    "The necessary changes to the algorithm are:\n",
    "1. Initialization\n",
    "\n",
    "   <div>$V(s) \\in \\mathbb{R}$ arbitrarily for all $s \\in S$</div>\n",
    "   <div>$\\pi(s) \\leftarrow $ random action from $A(s)$ with $p = \\frac{1}{|A(s)|}$</div>\n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "2. Policy Evaluation\n",
    "\n",
    "   <div>Loop:</div> \n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow 0$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>Loop for each $s \\in S:$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$v \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$\\Delta \\leftarrow max(\\Delta, |v - V(s)|)$</div>\n",
    "   <div>until $\\Delta < \\Theta$ (a small positive number determining the accuracy of estimation)</div> \n",
    "   <div>&nbsp;</div>\n",
    "   \n",
    "3. Policy Improvement\n",
    "\n",
    "   <div>$policy$-$stable \\leftarrow true$</div>\n",
    "   <div>For each $s \\in S$:</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$action \\leftarrow \\pi(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$old$-$value \\leftarrow V(s)$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$\\pi(s) \\leftarrow$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>$a_* = argmax_{a} \\sum_{s',r} p(s',r|s,a) [r + \\gamma V(s')]$, with $p = 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}$, or</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i>any $a \\neq a_*$, with $p = \\frac{\\epsilon}{|A(s)|}$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>$new$-$value \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V(s')]$</div>\n",
    "   <div><i>&nbsp;&nbsp;&nbsp;</i>If $old$-$action < \\pi(s)$ and $old$-$value < new$-$value$, then $policy$-$stable \\leftarrow false$</div>\n",
    "   <div>If $policy$-$stable$, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.7 (programming)\n",
    "\n",
    "## Question:\n",
    "Write a program for policy iteration and re-solve Jack's car rental problem with the following changes. One of Jack's employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car t the second location for free. Each additional car still costs \\\\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), than an additional cost of \\\\$4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of non-linearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem. \n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.8\n",
    "\n",
    "## Question:\n",
    "Why does the optimal policy for the gambler's problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.9 (programming)\n",
    "\n",
    "## Question:\n",
    "Implement value iteration for the gambler's problem and solve it for for $p_h = 0.25$ and $p_h = 0.55$. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital 0 or 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as $\\theta \\rightarrow 0$?  \n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.10\n",
    "\n",
    "## Question:\n",
    "What is the analog of the value iteration update (4.10) for action values, $q_{k+1}(s,a)$?\n",
    "\n",
    "## Answer:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
