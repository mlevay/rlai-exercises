{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1 \n",
    "  \n",
    "## Question:\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions,\n",
    "and rewards. Make the three examples as _different_ from each other as possible. The framework is abstract and\n",
    "flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "## Answer:\n",
    "Example 1: Hairdresser agent\n",
    "\n",
    "- States: the state of the hair and the desired cut of the client. The state of the hair and desired cut could be\n",
    "encoded as arrays of the length of the hair in a set of predetermined areas the head is divided in.\n",
    "- Actions: using the scissors or the clipper (and what accessory) and the area to be cut.\n",
    "- Rewards: negative rewards for the client complaints or imprecisions in the cut and positive rewards for tips.\n",
    "\n",
    "Example 2: DJ agent\n",
    "\n",
    "- States: a measure of how much people are dancing and singing to the song being played and the song currently playing.\n",
    "- Actions: given a setlist of 5000 songs, selecting the next song to be played (or a combination of them) and a type of\n",
    "transition between the songs.\n",
    "- Rewards: negative rewards given by people leaving the club faster than expected, positive rewards given by the level\n",
    "of danciness/_singiness_ in the club.\n",
    "\n",
    "Example 3: Texas Hold'em Poker player agent\n",
    "\n",
    "- States: The two cards in its hand and the cards showing in the table.\n",
    "- Actions: Check, call, raise, or fold.\n",
    "- Rewards: The money obtained or lost after playing one hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 \n",
    "  \n",
    "## Question:\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed learning tasks?\n",
    "Can you think of any clear exceptions?\n",
    "\n",
    "## Answer:\n",
    "We can try to think about it in terms of the limitations a finite MDP imposes in a problem definition and whether any\n",
    "approximations could exist within the framework.\n",
    "\n",
    "1) The Markov property, if not only the previous state and action selected influence which will be the next state. An\n",
    "example of this could be a modified game of chess where the order in which the pieces were moved affects the possible\n",
    "moves. This could be encoded within the MDP, an state would be composed of the position of the pieces and when\n",
    "they have been moved (skyrocketing the amount of available states making it a more difficult learning environment).\n",
    "If the sequence information wasn't available (or only partially available) at the time of making a decision,\n",
    "information from the past that can affect the next state would be missing, breaking the Markov property.\n",
    "\n",
    "2) The action and state sets must be finite. Any problem with infinite available actions or states would need\n",
    "alternative representations, such as grouping them into subsets and use these sets. An example of this could be a\n",
    "problem where the states are the natural numbers and we have to define intervals (e.g. negative numbers, numbers in the\n",
    "range 0-25, 25-200, and 200-âˆž) or where the actions can be a string of any size and we restrict it to a discrete number\n",
    "of lengths (e.g. only generate strings with length 3, 5, 8, 13, 21 and 34).\n",
    "\n",
    "3) Rewards must be numerical. If the rewards the environment gives back are not numerical we would need to encode them\n",
    "as numbers. This can be a highly difficult task as the rewards may not translate naturally to numbers. For example,\n",
    "if the rewards were your family's verbal feedback on how good the meal you prepared was, it would be difficult to\n",
    "convert it into a number and capture correctly its intensity (e.g. What's a better feedback? \"It was really good\" or\n",
    "\"I have enjoyed the meal a lot\"). If we opt for a simpler reward encoding, distinguishing only negative, neutral and\n",
    "positive comments, this may not capture perfectly the information given.\n",
    "\n",
    "The first example is, as far as I understand, a clear exception of the MDP-framework not being an appropriate\n",
    "representation. Apart from this, some of the points mentioned (or their combination) may difficult largely the task for\n",
    "an agent, making it really hard to learn anything valuable from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3\n",
    "  \n",
    "## Question:\n",
    "Consider the problem of driving. You could define the actions in\n",
    "terms of the accelerator, steering wheel, and brake, that is, where your body meets\n",
    "the machine. Or you could define them farther out - say, where the rubber meets the\n",
    "road, considering your actions to be tire torques. Or you could define them farther\n",
    "in - say, where your brain meets your body, the actions being muscle twitches to\n",
    "control your limbs. Or you could go to a really high level and say that your actions\n",
    "are your choices of where to drive. What is the right level, the right place to draw\n",
    "the line between agent and environment? On what basis is one location of the line\n",
    "to be preferred over another? Is there any fundamental reason for preferring one\n",
    "location over another, or is it a free choice?\n",
    "\n",
    "## Answer:\n",
    "The limit of the actions should be at the functional point, being the \n",
    "point at which is the agent makes the decision to take an action that\n",
    "the action always occurs in the same way every time. Above that point\n",
    "it is better to think of the agent as having sub-goals and goals, where\n",
    "something like walking to the door is a goal with the sub-goals of \n",
    "moving the legs, with the action of applying hydraulic pressure to\n",
    "particular points.\n",
    "\n",
    "However this does depend on the reliability of the system, and what\n",
    "reliability you require. If 99% of sub-goals are successful then it may\n",
    "be easy to convert them into actions. Abstracting in this way makes\n",
    "reaching larger goals easier as it dramatically reduces the\n",
    "action-space needed to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4\n",
    "\n",
    "## Question:\n",
    "Give a table analogous to that in Example 3.3, but for $p(s',r|s, a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s',r|s, a) > 0$.\n",
    "\n",
    "## Answer:\n",
    "Since there is a single reward defined for each triplet $(s, a, s')$, the table is the same filtering the lines with $p(s'|s,a)=0$.\n",
    "\n",
    "This fulfills the formula (3.4):\n",
    "\\begin{equation*}\n",
    "p(s'|s, a) = \\sum_{r \\in R} p(s',r|s, a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.5\n",
    "\n",
    "## Question:\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to\n",
    "apply to episodic tasks. Show that you know the modifications needed by giving the modified version\n",
    "of (3.3)\n",
    "\n",
    "## Answer:\n",
    "The original formula is:\n",
    "    \n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S, a \\in A(s)\n",
    "\\end{equation*}\n",
    "\n",
    "according to the definitions in 3.3, for episodic tasks the set of terminal and non-terminal states can be denoted as $S^+$. Therefore, the formula changes to:\n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S^+, a \\in A(s)\n",
    "\\end{equation*}\n",
    "as the dynamics of the MDP in an episodic task include as a possible transition those ending in a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.6\n",
    "  \n",
    "## Question:\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted,\n",
    "continuing formulation of this task?\n",
    "\n",
    "## Answer:\n",
    "\n",
    "The formula would change to:\n",
    "\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "In the limit (very large $T$), both returns would be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.7\n",
    "  \n",
    "## Question:\n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero at all\n",
    "other times. The task seems to break down naturally into episodes - the successive\n",
    "runs through the maze - so you decide to treat it as an episodic task, where the goal\n",
    "is to maximize expected total reward (3.1). After running the learning agent for a\n",
    "while, you find that it is showing no improvement in escaping from the maze. What\n",
    "is going wrong? Have you effectively communicated to the agent what you want it\n",
    "to achieve?\n",
    "\n",
    "## Answer:\n",
    "This is likely an exploration issue where the agent is unable to find\n",
    "the exit the first time and therefore doesn't know there's anything\n",
    "better than 0 as a reward. Potential solutions include having each\n",
    "non-goal state be worth -1, and/or extending the episode length. This\n",
    "would mean states the agents visits a lot (particularly around the start)\n",
    "will get worse and worse values so it will want to move away from there\n",
    "and eventually find the goal (essentially reaching the goal stops it\n",
    "being in pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.8\n",
    "\n",
    "## Question: \n",
    "Suppose $\\gamma$=0.5 and the following sequence of rewards is received $R_1$ = -1, $R_2$ = 2, $R_3$ = 6, $R_4$ = 3, and $R_5$ = 2, with $T$ = 5. What are $G_0$, $G_1$, ... , $G_5$? Hint: Work backwards. \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_0 = 2.0\n",
      "G_1 = 6.0\n",
      "G_2 = 8.0\n",
      "G_3 = 4.0\n",
      "G_4 = 2.0\n"
     ]
    }
   ],
   "source": [
    "r = [-1, 2, 6, 3, 2]\n",
    "gamma = 0.5\n",
    "\n",
    "for g_i in range(len(r)):\n",
    "    ret = sum([gamma**i * r_i for i, r_i in enumerate(r[g_i:])])    \n",
    "    print(\"G_\"+ str(g_i) + \" = \" + str(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.9\n",
    "\n",
    "## Question:\n",
    "Suppose $\\gamma$=0.9 and the reward sequence is $R_1$=2 followed by an infinite sequence of 7s. What are $G_1$ and $G_0$?\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "G_0 = 2 + 7 \\frac{1}{1 - \\gamma} = 72\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "G_1 = 7 + 7 \\frac{1}{1 - \\gamma} = 77\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.10\n",
    "\n",
    "## Question:\n",
    "Prove (3.10)\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^\\infty y_k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) = lim_{n \\rightarrow \\infty} \\frac{(1 +   \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} = lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} = \\frac{1}{1 - \\gamma}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.11\n",
    "\n",
    "## Question:\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function $p$ (3.2)?\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E} [R_{t+1} | S_t=s, \\pi] = \\sum_a \\pi(a|s) \\sum_{s',r} r\\ p(s', r|s, a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.12\n",
    "## Question:\n",
    "Give an equation for $v_{\\pi}$ in terms of $q_{\\pi}$ and $\\pi$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) = \\sum_{a\\in A(s)} q_\\pi(s,a) \\pi(a|s)\n",
    "\\end{equation*}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.13\n",
    "\n",
    "## Question:\n",
    "Give an equation for $q_{\\pi}$ in terms of $v_{\\pi}$ and the four-argument $p$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\cdot v_{\\pi}(s') \\big]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.14\n",
    "\n",
    "## Question:\n",
    "The Bellman equation (3.14) must hold for each state for the value function $v_{\\pi}$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4 and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&v_\\pi(s_{33}) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_\\pi(s')] \\\\ \n",
    "&v_\\pi(s_{33}) = \\frac{1}{4} \\cdot 1 \\cdot (0+0.9 \\cdot 2.3) + \\frac{1}{4} \\cdot 1 \\cdot (0+0.9 \\cdot 0.4) + \\frac{1}{4} \\cdot 1 \\cdot (0+0.9 \\cdot (\\text{-}0.4)) + \\frac{1}{4} \\cdot 1 \\cdot (0+0.9 \\cdot 0.7) \\\\\n",
    "&v_\\pi(s_{33}) = 0.9 \\cdot \\frac{1}{4} \\cdot (2.3+0.4-0.4+0.7) \\\\ \n",
    "&v_\\pi(s_{33}) = \\frac{0.9 \\cdot 3}{4} \\\\ \n",
    "&v_\\pi(s_{33}) = \\frac{27}{4} \\approx 0.7\n",
    "\\end{split}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.15\n",
    "\n",
    "## Question:\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus\n",
    "does not affect the relative values of any states under any policies. What is $v_c$ in terms of $c$ and $\\gamma$?\n",
    "\n",
    "\n",
    "## Answer:\n",
    "Only the intervals between them matter.\n",
    "\\begin{equation*}\n",
    "G'_t = \\sum_{k=0}^\\infty \\gamma^k (R_{t+k+1}+c) \\\\ = \\sum_{k=0}^\\infty \\gamma^k \\cdot R_{t+k+1} + \\sum_{k=0}^\\infty \\gamma^k \\cdot c = G_t + c \\cdot \\sum_{k=0}^\\infty \\gamma^k = G_t + \\frac{c}{1-\\gamma} \\\\\n",
    "\\implies v_c \\doteq \\frac{c}{1-\\gamma}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.16\n",
    "\n",
    "## Question:\n",
    "Now consider adding a constant $c$ to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "G'_t = \\sum_{k=t+1}^T \\gamma^{k-t-1} (R_k+c) \\\\ \n",
    "= \\sum_{k=t+1}^T \\gamma^{k-t-1} \\cdot R_k + \\sum_{k=t+1}^T \\gamma^{k-t-1} \\cdot c = G_t + c \\cdot \\sum_{k=t+1}^T \\gamma^{k-t-1} = G_t + c \\cdot f(t) \\\\\n",
    "\\implies v_c \\doteq c \\cdot f(t) \\text{ with } f(t) = \\sum_{k=t+1}^T \\gamma^{k-t-1} \\\\\n",
    "\\end{equation*}\n",
    "In the above, $f(t)$ is a decreasing function for $t > 0$ $\\implies$ rewards would vary dependent on $t$, with $v_c$ increasing with lower values of $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.17\n",
    "\n",
    "## Question:\n",
    "What is the Bellman equation for action values, that is, for $q_{\\pi}$? It must give the action value $q_{\\pi}(s, a)$ in terms of the action values, $q_{\\pi}(s',a')$, of possible successors to the stateâ€“action pair $(s, a)$. Hint: the backup diagram below corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.\n",
    "<img src=\"img/317.png\" style=\"height:200px\">\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\mathbb{E}[G_t | s_t=s, a_t=a] \n",
    "= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | s_t = s, a_t = a]\n",
    "\\\\ = \\sum_{s'} \\sum_r p(s',r|s,a) [r + \\gamma \\mathbb{E}_{\\pi}(G_{t+1} | s_{t+1} = s', a_{t+1} = a')]\n",
    "\\\\ = \\sum_{s',r} p(s',r|s,a) [r + \\gamma \\sum_{a'} \\pi(a'|s') \\cdot q_{\\pi}(s',a')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.18\n",
    "\n",
    "## Question:\n",
    "The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    "\n",
    "<img src=\"img/318.png\" style=\"height:120px\">\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the value at the root node, $v_{\\pi}(s)$, in terms of the value at the expected leaf node, $q_{\\pi}(s, a)$, given $S_t=s$. This equation should include an expectation conditioned on following the policy, $\\pi$. Then give a second equation in which the expected value is written out explicitly in terms of $\\pi(a|s)$ such that no expected value notation appears in the equation.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(s) = \\mathbb{E}[q_{\\pi}(s,a) | s_t = s]\n",
    "\\\\ = \\sum_a \\pi(a|s) \\cdot q_{\\pi}(s,a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.19\n",
    "\n",
    "## Question:\n",
    "The value of an action, $q_{\\pi}(s, a)$, depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (stateâ€“action pair) and branching to the possible next states:\n",
    "\n",
    "<img src=\"img/319.png\" style=\"height:120px\">\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the action value, $q_{\\pi}(s, a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value, $v_{\\pi}(S_{t+1})$, given that $S_t=s$ and $A_t=a$. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of $p(s',r|s, a)$ defined by (3.2), such that no expected value notation appears in the equation.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(s_{t+1}) | s_t = s, a_t = a]\n",
    "\\\\ = \\sum_{s',r} p(s',r | s,a) \\cdot [r + \\gamma v_{\\pi}(s')] \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.20\n",
    "\n",
    "## Question:\n",
    "Draw or describe the optimal state-value function for the golf example.\n",
    "\n",
    "## Answer:\n",
    "Some assumptions must be made and the incomplete problem definition emphasized:\n",
    "- We will assume that the probability of failure to cross a contour as intended (while putting or driving) equals zero.\n",
    "- We will show that, since the action \"sandwedge\" (or any other club that could get us out of a sand trap) is not defined, any sequence of actions that doesn't rule out hitting a sand trap will have a state value of -infinity\n",
    "\n",
    "The available actions are <b>pt</b> (use the putter) and <b>dr</b> (use the driver). We will delineate and name the possible states (across both possible actions) as follows:\n",
    "<table>\n",
    "    <tr style=\"font-weight:bold\">\n",
    "        <td>State</td>\n",
    "        <td>Description</td>\n",
    "        <td>Value (using putter)</td>\n",
    "        <td>Value (using driver)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_0$</td>\n",
    "        <td>Starting state (location of the tee)</td>\n",
    "        <td>-6</td>\n",
    "        <td>-3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_1$</td>\n",
    "        <td>When using the driver only, <br>all locations with value=-2 <br>except the green and the sand traps</td>\n",
    "        <td>(n/a)</td>\n",
    "        <td>-2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_2$</td>\n",
    "        <td>When putting only, all locations with value=-5</td>\n",
    "        <td>-5</td>\n",
    "        <td>(n/a)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_3$</td>\n",
    "        <td>When putting only, all locations with value=-4</td>\n",
    "        <td>-4</td>\n",
    "        <td>(n/a)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_4$</td>\n",
    "        <td>When putting only, all locations with value=-3</td>\n",
    "        <td>-3</td>\n",
    "        <td>(n/a)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_5$</td>\n",
    "        <td>When putting only, all locations with value=-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>(n/a)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_6$</td>\n",
    "        <td>The outer green</td>\n",
    "        <td>-1</td>\n",
    "        <td>-2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_7$</td>\n",
    "        <td>The inner green</td>\n",
    "        <td>-1</td>\n",
    "        <td>-1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_{st}$</td>\n",
    "        <td>All locations within sand traps <br>(terminal location due to lack of way out)</td>\n",
    "        <td>-$\\infty$</td>\n",
    "        <td>-$\\infty$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$s_T$</td>\n",
    "        <td>The hole (terminal location)</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A backup diagram of the optimal state-value function can be visualized as follows:\n",
    "<img src=\"img/320.png\" style=\"height:220px\">\n",
    "\n",
    "\\begin{equation*}\n",
    "v_*(s_7) = -1 = p(s_T|s_7, pt) \\cdot [r(s_7, pt) + \\gamma v_*(s_T)] \\implies \\gamma=1 \\text{ (since } v_*(s_T) = 0 \\text{)} \n",
    "\\\\ v_*(s_1) = p(s_7|s_1, dr) \\cdot [r(s_1, dr) + \\gamma v_*(s_7)]  = 2\n",
    "\\\\ v_*(s_0) = p(s_1|s_0, dr) \\cdot [r(s_0, dr) + \\gamma v_*(s_1)] + p(s_6|s_0, dr) \\cdot [r(s_0, dr) + \\gamma v_*(s_6)] + p(s_st|s_0, dr) \\cdot [r(s_0, dr) + \\gamma v_*(s_st)] \n",
    "\\\\ = p(s_1|s_0, dr) \\cdot (-3) + p(s_6|s_0, dr) \\cdot (-3) + p(s_st|s_0, dr) \\cdot (\\text{-} \\infty)\n",
    "\\\\ = \\text{-} \\infty\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.21\n",
    "\n",
    "## Question:\n",
    "Draw or describe the contours of the optimal action-value function for putting, $q_*(s,putter)$, for the golf example.\n",
    "\n",
    "## Answer:\n",
    "See previous exercise's answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.22\n",
    "\n",
    "## Question:\n",
    "Consider the continuing MDP shown below. \n",
    "<img src=\"img/322.png\" style=\"height:180px\">\n",
    "The only decision to be made is that in the top state, where two actions are available, $left$ and $right$. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, $\\pi_{left}$ and $\\pi_{right}$. What policy is optimal if $\\gamma=0$? If $\\gamma=0.9$? If $\\gamma=0.5$?\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "G_t = R_{t+1} + \\gamma G_{t+1}\n",
    "\\\\ \\pi_{left}: G_0 = 1 + \\gamma G_1 = 1 + \\gamma (0 + \\gamma G_2) = 1 + \\gamma^2 G_2\n",
    "\\\\ \\pi_{right}: G_0 = 0 + \\gamma G_1 = 0 + \\gamma (2 + \\gamma G_2) = 2 \\gamma + \\gamma^2 G_2\n",
    "\\\\ \\gamma = 0 \\implies \\pi_{left} > \\pi_{right}\n",
    "\\\\ \\gamma = 0.9 \\implies \\pi_{left} < \\pi_{right}\n",
    "\\\\ \\gamma = 0.5 \\implies \\pi_{left} = \\pi_{right}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.23\n",
    "\n",
    "## Question:\n",
    "Give the Bellman equation for $q_{\\pi}$ for the recycling robot.\n",
    "\n",
    "## Answer:\n",
    "The probabilities and reqards can be presented tabularly as follows, assuming $S = {h,l}$, $A = {s,w,re}$:\n",
    "<table>\n",
    "    <tr style=\"font-weight:bold\">\n",
    "        <td>State (s)</td>\n",
    "        <td>Action (a)</td>\n",
    "        <td>Next state (s')</td>\n",
    "        <td>Reward (r)</td>\n",
    "        <td style=\"width:120px\">Dynamics (p)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>h</td>\n",
    "        <td>s</td>\n",
    "        <td>h</td>\n",
    "        <td>$r_s$</td>\n",
    "        <td>$p(h|h,s) = \\alpha$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>h</td>\n",
    "        <td>s</td>\n",
    "        <td>l</td>\n",
    "        <td>$r_s$</td>\n",
    "        <td>$p(l|h,s) = 1-\\alpha$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l</td>\n",
    "        <td>s</td>\n",
    "        <td>h</td>\n",
    "        <td>-3</td>\n",
    "        <td>$p(h|l,s) = 1-\\beta$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l</td>\n",
    "        <td>s</td>\n",
    "        <td>l</td>\n",
    "        <td>$r_s$</td>\n",
    "        <td>$p(l|l,s) = \\beta$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>h</td>\n",
    "        <td>w</td>\n",
    "        <td>h</td>\n",
    "        <td>$r_w$</td>\n",
    "        <td>$p(h|h,w) = 1$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l</td>\n",
    "        <td>w</td>\n",
    "        <td>l</td>\n",
    "        <td>$r_s$</td>\n",
    "        <td>$p(l|l,w) = 1$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l</td>\n",
    "        <td>re</td>\n",
    "        <td>h</td>\n",
    "        <td>0</td>\n",
    "        <td>$p(h|l,re) = 1$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Optimal state-action value (general form):\n",
    "\\begin{equation*}\n",
    "q_*(s,a) = \\mathbb{E}[G_t|S_t=s, A_t=a] = \\mathbb{E}[R_{t+1} + \\gamma v_*(s') | G_t=s, A_t = a] = \\sum_{s',r} p(s',r|s,a) [r+\\gamma \\max_{a'} q_*(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Optimal state-action equations for the recycling robot:\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\\\\n",
    "&q_*(h,s) = \\alpha \\cdot \\Big[r_s + \\gamma \\cdot max \\big(q_*(h,s), q_*(h,w)\\big)\\Big] + (1-\\alpha) \\cdot \\Big[r_s + \\gamma \\cdot max\\big(q_*(l,s), q_*(l,w), q_*(l,re)\\big)\\Big]\n",
    "\\\\\n",
    "&q_*(h,w) = r_w + \\gamma \\cdot max\\big(q_*(h,s), q_*(h,w)\\big)\n",
    "\\\\\n",
    "&q_*(l,s) = \\beta \\cdot \\Big[r_s + \\gamma \\cdot max\\big(q_*(l,s), q_*(l,w), q_*(l,re)\\big)\\Big] + (1-\\beta) \\cdot \\gamma \\cdot max\\big(q_*(h,s), q_*(h,w)\\big)\n",
    "\\\\\n",
    "&q_*(l,w) = r_w + \\gamma \\cdot max\\big(q_*(l,s), q_*(l,w), q_*(l,re)\\big)\n",
    "\\\\\n",
    "&q_*(l,re) = \\gamma \\cdot max\\big(q_*(h,s), q_*(h,w)\\big)\n",
    "\\\\\n",
    "\\end{split}\n",
    "\\\\\n",
    "\\end{equation*}\n",
    "For any choice of $r_{s}$, $r_{w}$, $\\alpha$, $\\beta$, and $\\gamma$, with 0 <= $\\gamma$ < 1, 0 <= $\\alpha$, $\\beta$ <= 1, there is exactly one set of numbers, $q_{*}$(h,s), $q_{*}$(h,w), $q_{*}$(l,s), $q_{*}$(l,w), and $q_{*}$(l,re), that simultaneiusly satisfy these five non-linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.24\n",
    "\n",
    "## Question:\n",
    "Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n",
    "\n",
    "## Answer:\n",
    "The only states we need to condsider when computing $s_{A}$ are $S^*$ $=$ $\\{$ $s_{A}$, $s_{22}$, $s_{32}$, $s_{42}$ $s_{A'}$ $\\}$, where we use $s_{A}$ and $s_{A'}$ as an alias for $s_{12}$ and $s_{A'}$, respectively (the latter two following a 1-based matrix index numenclature).\n",
    "\n",
    "General form for Bellman's state optimal value equation:\n",
    "\\begin{equation*}\n",
    "v_*(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a) \\cdot [r + \\gamma \\cdot v(s')]\n",
    "\\end{equation*}\n",
    "\n",
    "We proceed with solving the system of non-linear equations for $S^*$: \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&v_*(s_{A'}) = 0.9 \\cdot v_*(s_{42}) \\\\\n",
    "&v_*(s_{42}) = 0.9 \\cdot v_*(s_{32}) \\\\\n",
    "&v_*(s_{32'}) = 0.9 \\cdot v_*(s_{22}) \\\\\n",
    "&v_*(s_{22'}) = 0.9 \\cdot v_*(s_{A}) \\\\\n",
    "&v_*(s_{A}) = max \\big( r_{A'} + 0.9 \\cdot v_*(s_{A'}), r_{penalty} + 0.9 \\cdot v_*(s_{A}) \\big)\n",
    "\\end{split}\n",
    "\\begin{split}\n",
    "\\implies\n",
    "\\end{split}\n",
    "\\begin{split}\n",
    "&v_*(s_{A'}) = 0.9^{4} \\cdot v_*(s_{A}) \\\\\n",
    "&v_*(s_{A}) = max \\big( 10 + 0.9 \\cdot v_*(s_{A'}), -1 + 0.9 \\cdot v_*(s_{A}) \\big)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "The maximizing value for $v_*(s_{A})$, computed to three decimal places, is therefore:\n",
    "\\begin{equation*}\n",
    "v_*(s_{A}) = 10 + 0.9^{5} \\cdot v_*(s_{A}) \\implies v_*(s_{A}) = 24.419\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.25\n",
    "\n",
    "## Question:\n",
    "Give an equation for $v_*$ in terms of $q_*$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "v_*(s) = \\max_{a} q_*(s,a) , \\forall s \\in S\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.26\n",
    "\n",
    "## Question:\n",
    "Give an equation for $q_*$ in terms of $v_*$ and the four-argument $p$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "q_*(s,a) = \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\cdot v_*(s') \\big] , \\forall s \\in S, a \\in A\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.27\n",
    "\n",
    "## Question:\n",
    "Give an equation for $\\pi_*$ in terms of $q_*$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "\\pi_*(s) = \\arg \\max_{a} q_*(s,a) , \\forall s \\in S\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.28\n",
    "\n",
    "## Question:\n",
    "Give an equation for $\\pi_*$ in terms of $v_*$ and the four-argument $p$.\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "\\pi_*(s) = \\arg \\max_{a} \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma \\cdot v_*(s') \\big] , \\forall s \\in S\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.29\n",
    "\n",
    "## Question:\n",
    "Rewrite the four Bellman equations for the four value functions $(v_{\\pi},v_*,q_{\\pi},and q_*)$ in terms of the three-argument function $p$ (3.4) and the two-argument function $r$ (3.5).\n",
    "\n",
    "## Answer:\n",
    "\\begin{equation*}\n",
    "v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\Big( r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\cdot v_{\\pi}(s') \\Big) \\\\\n",
    "v_*(s) = \\max_{a} \\Big( r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\cdot v_*(s') \\Big) \\\\\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "q_{\\pi}(s,a) = r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\cdot v_{\\pi}(s') = r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\sum_{a' \\in A} \\pi(a'|s') \\cdot q_{\\pi} (s',a') \\\\\n",
    "q_*(s,a) = r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\cdot v_*(s') = r(s,a) + \\gamma \\cdot \\sum_{s' \\in S} p(s'|s,a) \\cdot \\max_{a'} q_*(s',a') \\\\\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
